{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field, InitVar\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CheckBoardDataset(Dataset):\n",
    "    samples: int\n",
    "    rows: int = 4\n",
    "    cols: int = 4\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.samples\n",
    "\n",
    "    def sample_x0(self):\n",
    "        return np.random.randn(2)\n",
    "\n",
    "    def sample_x1(self):\n",
    "        x = np.random.uniform(-self.cols // 2, self.cols // 2)\n",
    "        y = np.random.uniform(-self.rows // 2, self.rows // 2)\n",
    "        y = np.where((np.floor(x) + np.floor(y)) % 2 == 0, y, -y)\n",
    "        return np.array([x, y])\n",
    "\n",
    "    def sample_t(self):\n",
    "        return np.random.rand(1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x0 = self.sample_x0().astype(np.float32)\n",
    "        x1 = self.sample_x1().astype(np.float32)\n",
    "        t = self.sample_t().astype(np.float32)\n",
    "        return x0, x1, t\n",
    "\n",
    "    def loader(self, batch_size: int):\n",
    "        return DataLoader(self, batch_size=batch_size, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from lightning import LightningModule, Trainer\n",
    "from lightning.pytorch import callbacks\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, dim, act:nn.Module):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.mu = nn.Linear(dim, dim, bias=False)\n",
    "        self.logsigma = nn.Linear(dim, dim, bias=False)\n",
    "        self.lin1 = nn.Linear(dim, dim)\n",
    "        self.act = act\n",
    "        self.lin2 = nn.Linear(dim, dim)\n",
    "\n",
    "    def __call__(self, x, c):\n",
    "        x = self.norm(x)\n",
    "        x = x*self.logsigma(c).exp() + self.mu(c)\n",
    "        h = self.lin2(self.act(self.lin1(x)))\n",
    "        return x + h\n",
    "\n",
    "\n",
    "class CNF(LightningModule):\n",
    "    def __init__(self, dim, hidden=256, num_blocks=4, act=nn.GELU()):\n",
    "        super().__init__()\n",
    "        self.in_proj = nn.Linear(dim, hidden)\n",
    "        self.t_proj = nn.Linear(1, hidden)\n",
    "        self.out_proj = nn.Linear(hidden, dim)\n",
    "        self.blocks = nn.ModuleList(ResBlock(hidden, act) for _ in range(num_blocks))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-5)\n",
    "\n",
    "    def __call__(self, t, x):\n",
    "        x = self.in_proj(x)\n",
    "        c = self.t_proj(t)\n",
    "        for block in self.blocks:\n",
    "            x = block(x, c)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x0, x1, t = batch\n",
    "        xt = t * x1 + (1 - t) * x0 # + sigma_min * t * x0\n",
    "\n",
    "        flow = self(t, xt)\n",
    "        target_flow = (x1 - x0) # +sigma_min * x0\n",
    "        loss = torch.nn.functional.mse_loss(flow, target_flow)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 3 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=3)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA RTX A6000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name     </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type       </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ in_proj  │ Linear     │    768 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ t_proj   │ Linear     │    512 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ out_proj │ Linear     │    514 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ blocks   │ ModuleList │  1.1 M │\n",
       "└───┴──────────┴────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName    \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType      \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ in_proj  │ Linear     │    768 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ t_proj   │ Linear     │    512 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ out_proj │ Linear     │    514 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ blocks   │ ModuleList │  1.1 M │\n",
       "└───┴──────────┴────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 1.1 M                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 1.1 M                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 4                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 1.1 M                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 1.1 M                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 4                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44231fa035f04fb88e26e8c418e7952f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = CNF(2)\n",
    "dataset = CheckBoardDataset(samples=128 * 1024)\n",
    "trainer = Trainer(\n",
    "    max_epochs=100,\n",
    "    callbacks=[\n",
    "        callbacks.RichProgressBar(),\n",
    "        callbacks.RichModelSummary(),\n",
    "        callbacks.StochasticWeightAveraging(0.01),\n",
    "    ],\n",
    ")\n",
    "trainer.fit(model, dataset.loader(batch_size=1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_foward(model, x, n_steps=10):\n",
    "    dt = 1 / n_steps\n",
    "    xt = [x]\n",
    "    for t in tqdm(torch.arange(0., 1., dt)):\n",
    "        t = t* torch.ones(x.shape[0], 1)\n",
    "        x = x + dt * model(t, x)\n",
    "        xt.append(x.detach())\n",
    "    return xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, x1, t = next(iter(dataset.loader(batch_size=64*1024)))\n",
    "xt = push_foward(model, x0, n_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 3))\n",
    "plt.subplot(131)\n",
    "plt.title('x0')\n",
    "plt.hist2d(*x0.T, bins=100)\n",
    "plt.subplot(132)\n",
    "plt.title('x1')\n",
    "plt.hist2d(*xt[-1].T, bins=100)\n",
    "plt.subplot(133)\n",
    "plt.title('target')\n",
    "plt.hist2d(*x1.T, bins=100)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(len(xt)*2, 2))\n",
    "for i, x in enumerate(xt):\n",
    "    plt.subplot(1, len(xt), i+1)\n",
    "    plt.title(f\"t={i/(len(xt)-1):.2f}\")\n",
    "    plt.hist2d(x[:, 0], x[:, 1], bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
